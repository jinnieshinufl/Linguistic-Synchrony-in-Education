# -*- coding: utf-8 -*-
"""aligner approach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLABoW5QeSiYFjBA5rhpuqWueduqnO7y
"""

import align, os
import pandas as pd
import time
import warnings

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')

import os,re,math,csv,string,random,logging,glob,itertools,operator,sys
from os import listdir, pathsep
from os.path import isfile, join
from collections import Counter, defaultdict, OrderedDict
from itertools import chain, combinations

import pandas as pd
import numpy as np
import scipy
from scipy import spatial

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from nltk.tag.stanford import StanfordPOSTagger
from nltk.util import ngrams

import gensim
from gensim.models import word2vec

dataframe = pd.read_excel('final_full_post_processed_equalized.xlsx')

dataframe.columns

for file in set(list(dataframe.filename)):
    temp = dataframe[dataframe.filename==file]
    with open('./new_data_files_input/'+str(file) +'.txt','w') as t:
        t.write('participant')
        t.write('\t')
        t.write('content')
        t.write('\n')
        for k, v in zip(temp.speaker.values, temp.text_processed.values):
            t.write(str(k))
            t.write('\t')
            t.write(str(v))
            t.write('\n')

def InitialCleanup(dataframe,
                   minwords=2,
                   use_filler_list=None,
                   filler_regex_and_list=False):

    """
    Perform basic text cleaning to prepare dataframe
    for analysis. Remove non-letter/-space characters,
    empty turns, turns below a minimum length, and
    fillers.

    By default, preserves turns 2 words or longer.
    If desired, this may be changed by updating the
    `minwords` argument.

    By default, remove common fillers through regex.
    If desired, ignore regex default and only remove
    user-specified words by passing a list of literal strings
    to `use_filler_list` argument, and if both regex default and
    list of additional literal strings are to be used,
    update `filler_regex_and_list=True`.
    """

    # only allow strings, spaces, and newlines to pass
    WHITELIST = string.ascii_letters + '\'' + ' '

    # remove inadvertent empty turns
    dataframe = dataframe[pd.notnull(dataframe['content'])]

    # remove turns where content is equal to NA (otherwise will throw a `SettingWithCopyWarning` warning)
    dataframe = dataframe.dropna(subset=['content'])

    # internal function: remove fillers via regular expressions
    def applyRegExpression(textFiller):
        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\s', ' ', textFiller) # at the start of a string
        textClean = re.sub('\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]+\s', ' ', textClean) # within a string
        textClean = re.sub('\s(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # end of a string
        textClean = re.sub('^(?!mom|am|ham)[u*|h*|m*|o*|a*]+[m*|h*|u*|a*]$', ' ', textClean) # if entire turn string
        return textClean

    ###########################
    ### NEW 06/20/22: Need to add as option: Before stripping non-ascii characters, removes any text within brackets or parentheses that are typical of transcribed texts
    # def regExRmTranscriptTags(textFiller):
    #     textClean = re.sub('\[(.*?)\]', ' ', textFiller) # any text within brackets
    #     textClean = re.sub('\((.*?)\)', ' ', textClean) # any text within parentheses
    #     return textClean

    # dataframe['content'] = dataframe['content'].apply(regExRmTranscriptTags)
    ###########################

    # create a new column with only approved text before cleaning per user-specified settings
    dataframe['clean_content'] = dataframe['content'].apply(lambda utterance: ''.join([char for char in utterance if char in WHITELIST]).lower())

    # DEFAULT: remove typical speech fillers via regular expressions (examples: "um, mm, oh, hm, uh, ha")
    if use_filler_list is None and not filler_regex_and_list:
        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)

    # OPTION 1: remove speech fillers or other words specified by user in a list
    elif use_filler_list is not None and not filler_regex_and_list:
        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(" ") if word not in use_filler_list]))

    # OPTION 2: remove speech fillers via regular expression and any additional words from user-specified list
    elif use_filler_list is not None and filler_regex_and_list:
        dataframe['clean_content'] = dataframe['clean_content'].apply(applyRegExpression)
        dataframe['clean_content'] = dataframe['clean_content'].apply(lambda utterance: ' '.join([word for word in utterance.split(" ") if word not in use_filler_list]))

    # OPTION 3: nothing is filtered
    else:
        dataframe['clean_content'] = dataframe['clean_content']

    # drop the old "content" column and rename the clean "content" column
    dataframe = dataframe.drop(['content'],axis=1)
    dataframe = dataframe.rename(index=str,
                                 columns ={'clean_content': 'content'})

    # remove rows that are now blank or do not meet `minwords` requirement, then drop length column
    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: word_tokenize(x)).str.len()
    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen < int(minwords)].index).drop(['utteranceLen'],axis=1)
    dataframe = dataframe.reset_index(drop=True)

    # return the cleaned dataframe
    return dataframe

def AdjacentMerge(dataframe):

    """
    Given a dataframe of conversation turns,
    merge adjacent turns by the same speaker.
    """

    repeat=1
    while repeat==1:
        l1=len(dataframe)
        DfMerge = []
        k = 0
        if len(dataframe) > 0:
            while k < len(dataframe)-1:
                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:
                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])
                    k = k + 1
                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:
                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + " " + dataframe['content'].iloc[k+1]])
                    k = k + 2
            if k == len(dataframe)-1:
                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])

        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))
        if l1==len(dataframe):
            repeat=0

    return dataframe

def Tokenize(text):

    """
    Given list of text to be processed and returns list
    (expands out common contractions in the process)
    """

    # expand out based on a fixed list of common contractions
    contract_dict = { "ain't": "is not",
        "aren't": "are not",
        "can't": "cannot",
        "can't've": "cannot have",
        "'cause": "because",
        "could've": "could have",
        "couldn't": "could not",
        "couldn't've": "could not have",
        "didn't": "did not",
        "doesn't": "does not",
        "don't": "do not",
        "hadn't": "had not",
        "hadn't've": "had not have",
        "hasn't": "has not",
        "haven't": "have not",
        "he'd": "he had",
        "he'd've": "he would have",
        "he'll": "he will",
        "he'll've": "he will have",
        "he's": "he is",
        "how'd": "how did",
        "how'd'y": "how do you",
        "how'll": "how will",
        "how's": "how is",
        "i'd": "i would",
        "i'd've": "i would have",
        "i'll": "i will",
        "i'll've": "i will have",
        "i'm": "i am",
        "i've": "i have",
        "isn't": "is not",
        "it'd": "it would",
        "it'd've": "it would have",
        "it'll": "it will",
        "it'll've": "it will have",
        "it's": "it is",
        "let's": "let us",
        "ma'am": "madam",
        "mayn't": "may not",
        "might've": "might have",
        "mightn't": "might not",
        "mightn't've": "might not have",
        "must've": "must have",
        "mustn't": "must not",
        "mustn't've": "must not have",
        "needn't": "need not",
        "needn't've": "need not have",
        "o'clock": "of the clock",
        "oughtn't": "ought not",
        "oughtn't've": "ought not have",
        "shan't": "shall not",
        "sha'n't": "shall not",
        "shan't've": "shall not have",
        "she'd": "she would",
        "she'd've": "she would have",
        "she'll": "she will",
        "she'll've": "she will have",
        "she's": "she is",
        "should've": "should have",
        "shouldn't": "should not",
        "shouldn't've": "should not have",
        "so've": "so have",
        "so's": "so as",
        "that'd": "that had",
        "that'd've": "that would have",
        "that's": "that is",
        "there'd": "there would",
        "there'd've": "there would have",
        "there's": "there is",
        "they'd": "they would",
        "they'd've": "they would have",
        "they'll": "they will",
        "they'll've": "they will have",
        "they're": "they are",
        "they've": "they have",
        "to've": "to have",
        "wasn't": "was not",
        "we'd": "we would",
        "we'd've": "we would have",
        "we'll": "we will",
        "we'll've": "we will have",
        "we're": "we are",
        "we've": "we have",
        "weren't": "were not",
        "what'll": "what will",
        "what'll've": "what will have",
        "what're": "what are",
        "what's": "what is",
        "what've": "what have",
        "when's": "when is",
        "when've": "when have",
        "where'd": "where did",
        "where's": "where is",
        "where've": "where have",
        "who'll": "who will",
        "who'll've": "who will have",
        "who's": "who is",
        "who've": "who have",
        "why's": "why is",
        "why've": "why have",
        "will've": "will have",
        "won't": "will not",
        "won't've": "will not have",
        "would've": "would have",
        "wouldn't": "would not",
        "wouldn't've": "would not have",
        "y'all": "you all",
        "y'all'd": "you all would",
        "y'all'd've": "you all would have",
        "y'all're": "you all are",
        "y'all've": "you all have",
        "you'd": "you would",
        "you'd've": "you would have",
        "you'll": "you will",
        "you'll've": "you will have",
        "you're": "you are",
        "you've": "you have" }
    contractions_re = re.compile('(%s)' % '|'.join(list(contract_dict.keys())))

    # internal function:
    def expand_contractions(text, contractions_re=contractions_re):
        def replace(match):
            return contract_dict[match.group(0)]
        return contractions_re.sub(replace, text.lower())

    # process all words in the text
    text = expand_contractions(text)
    cleantoken = word_tokenize(text)

    return cleantoken


def TokenizeSpell(text,
             nwords):

    """
    Given list of text to be processed and a list
    of known words, return a list of edited and
    tokenized words.

    Spell-checking is implemented using a
    Bayesian spell-checking algorithm
    (http://norvig.com/spell-correct.html).

    By default, this is based on the Project Gutenberg
    corpus, a collection of approximately 1 million texts
    (http://www.gutenberg.org). A copy of this is included
    within this package. If desired, users may specify a
    different spell-check training corpus in the
    `training_dictionary` argument of the
    `prepare_transcripts()` function.
    """

    # internal function: identify possible spelling errors for a given word
    def edits1(word):
        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        deletes    = [a + b[1:] for a, b in splits if b]
        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]
        replaces   = [a + c + b[1:] for a, b in splits for c in string.ascii_lowercase if b]
        inserts    = [a + c + b     for a, b in splits for c in string.ascii_lowercase]
        return set(deletes + transposes + replaces + inserts)

    # internal function: identify known edits
    def known_edits2(word,nwords):
        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)

    # internal function: identify known words
    def known(words,nwords): return set(w for w in words if w in nwords)

    # internal function: correct spelling
    def correct(word,nwords):
        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]
        return max(candidates, key=nwords.get)

    cleantoken = []
    token = Tokenize(text)

    for word in token:
        if "'" not in word:
            cleantoken.append(correct(word,nwords))
        else:
            cleantoken.append(word)
    return cleantoken


def pos_to_wn(tag):

    """
    Convert NLTK default tagger output into a format that Wordnet
    can use in order to properly lemmatize the text.
    """

    # create some inner functions for simplicity
    def is_noun(tag):
        return tag in ['NN', 'NNS', 'NNP', 'NNPS']
    def is_verb(tag):
        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']
    def is_adverb(tag):
        return tag in ['RB', 'RBR', 'RBS']
    def is_adjective(tag):
        return tag in ['JJ', 'JJR', 'JJS']

    # check each tag against possible categories
    if is_noun(tag):
        return wn.NOUN
    elif is_verb(tag):
        return wn.VERB
    elif is_adverb(tag):
        return wn.ADV
    elif is_adjective(tag):
        return wn.ADJ
    else:
        return wn.NOUN


def Lemmatize(tokenlist):
    lemmatizer = WordNetLemmatizer()
    defaultPos = nltk.pos_tag(tokenlist) # get the POS tags from NLTK default tagger
    words_lemma = []
    for item in defaultPos:
        words_lemma.append(lemmatizer.lemmatize(item[0],pos_to_wn(item[1]))) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize
    return words_lemma


def ApplyPOSTagging(df,
                    filename,
                    add_stanford_tags=False,
                    stanford_pos_path=None,
                    stanford_language_path=None):

    """
    Given a dataframe of conversation turns, return a new
    dataframe with part-of-speech tagging. Add filename
    (given as string) as a new column in returned dataframe.

    By default, return only tags from the NLTK default POS
    tagger. Optionally, also return Stanford POS tagger
    results by setting `add_stanford_tags=True`.

    If Stanford POS tagging is desired, specify the
    location of the Stanford POS tagger with the
    `stanford_pos_path` argument. Also note that the
    default language model for the Stanford tagger is
    English (english-left3words-distsim.tagger). To change
    language model, specify the location with the
    `stanford_language_path` argument.

    """

    # if desired, import Stanford tagger
    if add_stanford_tags:
        if stanford_pos_path is None or stanford_language_path is None:
            raise ValueError('Error! Specify path to Stanford POS tagger and language model using the `stanford_pos_path` and `stanford_language_path` arguments')
        else:
            stanford_tagger = StanfordPOSTagger(stanford_pos_path + stanford_language_path,
                                                stanford_pos_path + 'stanford-postagger.jar')

    # add new columns to dataframe
    df['tagged_token'] = df['token'].apply(nltk.pos_tag)
    df['tagged_lemma'] = df['lemma'].apply(nltk.pos_tag)

    # if desired, also tag with Stanford tagger
    if add_stanford_tags:
        df['tagged_stan_token'] = df['token'].apply(stanford_tagger.tag)
        df['tagged_stan_lemma'] = df['lemma'].apply(stanford_tagger.tag)

    df['file'] = filename

    # return finished dataframe
    return df

def prepare_transcripts(input_files,
                        output_file_directory,
                        run_spell_check=True,
                        training_dictionary=None,
                        minwords=2,
                        use_filler_list=None,
                        filler_regex_and_list=False,
                        add_stanford_tags=False,
                        stanford_pos_path=None,
                        stanford_language_path=None,
                        input_as_directory=True,
                        save_concatenated_dataframe=True):

    """
    Prepare transcripts for similarity analysis.

    Given individual .txt files of conversations,
    return a completely prepared dataframe of transcribed
    conversations for later ALIGN analysis, including: text
    cleaning, merging adjacent turns, spell-checking,
    tokenization, lemmatization, and part-of-speech tagging.
    The output serve as the input for later ALIGN
    analysis.

    Parameters
    ----------
    input_files : str (directory name) or list of str (file names)
        Raw files to be cleaned. Behavior governed by `input_as_directory`
        parameter as well.

    output_file_directory : str
        Name of directory where output for individual conversations will be
        saved.

    run_spell_check : boolean, optional (default: True)
        Specify whether to run the spell-checking algorithm (True) or to
        ignore it (False).

    training_dictionary : str, optional (default: None)
        Specify whether to train the spell-checking dictionary using a
        provided file name (str) or the default Project
        Gutenberg corpus [http://www.gutenberg.org] (None).

    minwords : int, optional (2)
        Specify the minimum number of words in a turn. Any turns with fewer
        than the minimum number of words will be removed from the corpus.
        (Note: `minwords` must be equal to or greater than `maxngram` provided
        to `calculate_alignment()` and `calculate_baseline_alignment` in later
        steps.)

    use_filler_list : list of str, optional (default: None)
        Specify whether words should be filtered from all conversations using a
        list of filler words (list of str) or using regular expressions to
        filter out common filler words (None). Behavior governed by
        `filler_regex_and_list` parameter as well.

    filler_regex_and_list : boolean, optional (default: False)
        If providing a list to `use_filler_list` parameter, specify whether to
        use only the provided list (False) or to use both the provided list and
        the regular expression filter (True).

    add_stanford_tags : boolean, optional (default: False)
        Specify whether to return part-of-speech similarity scores based on
        Stanford POS tagger in addition to the Penn POS tagger (True) or to
        return only POS similarity scores from the Penn tagger (False). (Note:
        Including Stanford POS tags will lead to a significant increase in
        processing time.)

    stanford_pos_path : str, optional (default: None)
        If Stanford POS tagging is desired, specify local path to Stanford POS
        tagger.

    stanford_language_path : str, optional (default: None)
        If Stanford POS tagging is desired, specify local path to Stanford POS
        tagger for the desired language (str) or use the default English tagger
        (None).

    input_as_directory : boolean, optional (default: True)
        Specify whether the value passed to `input_files` parameter should
        be read as a directory (True) or a list of files to be processed
        (False).

    save_concatenated_dataframe : boolean, optional (default: True)
        Specify whether to save the individual conversation output data only
        as individual files in the `output_file_directory` (False) or to save
        the individual files as well as a single concatenated dataframe (True).

    Returns
    -------
    prepped_df : Pandas DataFrame
        A single concatenated dataframe of all transcripts, ready for
        processing with `calculate_alignment()` and
        `calculate_baseline_alignment()`.

    """
    if run_spell_check == True:

        # create an internal function to train the spell-checking model
        def train(features):
            model = defaultdict(lambda: 1)
            for f in features:
                model[f] += 1
            return model

        # if no training dictionary is specified, use the Gutenberg corpus
        if training_dictionary is None:

            # first, get the name of the package directory
            #module_path = os.path.dirname('gutenberg.txt')

            # then construct the path to the text file
            training_dictionary = os.path.dirname('./gutenberg.txt')


        # train our spell-checking model
        nwords = train(re.findall('[a-z]+', (open(training_dictionary).read().lower())))

    # grab the appropriate files
    if not input_as_directory:
        file_list = glob.glob(input_files)
    else:
        file_list = glob.glob(input_files+"/*.txt")

    # cycle through all files
    tmpfiles = list()

    for fileName in file_list:

        # let us know which file we're processing
        print(("Processing: "+fileName))
        dataframe = pd.read_csv(fileName, sep='\t',encoding='utf-8')

        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag
        dataframe = InitialCleanup(dataframe,
                                   minwords=minwords,
                                   use_filler_list=use_filler_list,
                                   filler_regex_and_list=filler_regex_and_list)
        dataframe = AdjacentMerge(dataframe)

        # tokenize and lemmatize
        if run_spell_check == True:
            dataframe['token'] = dataframe['content'].apply(TokenizeSpell,
                                            args=(nwords,))
        else:
            dataframe['token'] = dataframe['content'].apply(Tokenize)

        dataframe['lemma'] = dataframe['token'].apply(Lemmatize)

        # apply part-of-speech tagging
        dataframe = ApplyPOSTagging(dataframe,
                                    filename=os.path.basename(fileName),
                                    add_stanford_tags=add_stanford_tags,
                                    stanford_pos_path=stanford_pos_path,
                                    stanford_language_path=stanford_language_path)

        # export the conversation's dataframe as a CSV
        conversation_file = os.path.join(output_file_directory,os.path.basename(fileName))
        dataframe.to_csv(conversation_file, encoding='utf-8',index=False,sep='\t')

        tmpfiles.append(dataframe)

    prepped_df = pd.concat(tmpfiles)

    # save the concatenated dataframe
    if save_concatenated_dataframe:
        concatenated_file = os.path.join(output_file_directory,'./align_concatenated_dataframe.txt')
        prepped_df.to_csv(concatenated_file,
                    encoding='utf-8',index=False, sep='\t')

    # return the dataframe
    return prepped_df

model_store = prepare_transcripts('./new_data_files_input',
                        './new_data_files_output',
                        run_spell_check=True,
                        training_dictionary='./gutenberg.txt',
                        minwords=2,
                        use_filler_list=None,
                        filler_regex_and_list=False,
                        add_stanford_tags=False,
                        stanford_pos_path=None,
                        stanford_language_path=None,
                        input_as_directory=True,
                        save_concatenated_dataframe=True)

PREPPED_TRANSCRIPTS = os.path.join(
                                   '/Users/jinnie.shin/Downloads/new_data_files_ouput/')
ANALYSIS_READY = os.path.join('./data_files_analysis')
PRETRAINED_INPUT_FILE = os.path.join('./GoogleNews-vectors-negative300.bin')

# set standards to be used for real and surrogate
INPUT_FILES = PREPPED_TRANSCRIPTS
MAXNGRAM = 2
USE_PRETRAINED_VECTORS = True
SEMANTIC_MODEL_INPUT_FILE = './align_concatenated_dataframe.txt'
PRETRAINED_FILE_DIRECTORY = PRETRAINED_INPUT_FILE
ADD_STANFORD_TAGS = False
IGNORE_DUPLICATES = True
HIGH_SD_CUTOFF = 3
LOW_N_CUTOFF = 1

[turn_real,convo_real] = calculate_alignment(
                            input_files=INPUT_FILES,
                            maxngram=MAXNGRAM,
                            use_pretrained_vectors=USE_PRETRAINED_VECTORS,
                            pretrained_input_file=PRETRAINED_INPUT_FILE,
                            semantic_model_input_file=SEMANTIC_MODEL_INPUT_FILE,
                            output_file_directory=ANALYSIS_READY,
                            add_stanford_tags=ADD_STANFORD_TAGS,
                            ignore_duplicates=IGNORE_DUPLICATES,
                            high_sd_cutoff=HIGH_SD_CUTOFF,
                            low_n_cutoff=LOW_N_CUTOFF)

turn_real.to_csv('turn_real.csv')
convo_real.to_csv('convo_real.csv')

import os,re,math,csv,string,random,logging,glob,itertools,operator,sys
from os import listdir
from os.path import isfile, join
from collections import Counter, defaultdict, OrderedDict
from itertools import chain, combinations

import pandas as pd
import numpy as np
import scipy
from scipy import spatial

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from nltk.tag.stanford import StanfordPOSTagger
from nltk.util import ngrams

import gensim
from gensim.models import word2vec

def ngram_pos(sequence1,sequence2,ngramsize=2,
                   ignore_duplicates=True):
    """
    Remove mimicked lexical sequences from two interlocutors'
    sequences and return a dictionary of counts of ngrams
    of the desired size for each sequence.

    By default, consider bigrams. If desired, this may be
    changed by setting `ngramsize` to the appropriate
    value.

    By default, ignore duplicate lexical n-grams when
    processing these sequences. If desired, this may
    be changed with `ignore_duplicates=False`.
    """

    # remove duplicates and recreate sequences
    sequence1 = set(ngrams(sequence1,ngramsize))
    sequence2 = set(ngrams(sequence2,ngramsize))

    # if desired, remove duplicates from sequences
    if ignore_duplicates:
        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence1 - sequence2)]
        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in list(sequence2 - sequence1)]
    else:
        new_sequence1 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence1]
        new_sequence2 = [tuple([''.join(pair[1]) for pair in tup]) for tup in sequence2]

    # return counters
    return Counter(new_sequence1), Counter(new_sequence2)


def ngram_lexical(sequence1,sequence2,ngramsize=2):
    """
    Create ngrams of the desired size for each of two
    interlocutors' sequences and return a dictionary
    of counts of ngrams for each sequence.

    By default, consider bigrams. If desired, this may be
    changed by setting `ngramsize` to the appropriate
    value.
    """

    # generate ngrams
    sequence1 = list(ngrams(sequence1,ngramsize))
    sequence2 = list(ngrams(sequence2,ngramsize))

    # join for counters
    new_sequence1 = [' '.join(pair) for pair in sequence1]
    new_sequence2 = [' '.join(pair) for pair in sequence2]

    # return counters
    return Counter(new_sequence1), Counter(new_sequence2)


def get_cosine(vec1, vec2):
    """
    Derive cosine similarity metric, standard measure.
    Adapted from <https://stackoverflow.com/a/33129724>.
    """

    # NOTE: results identical to sklearn class `cosine_similarity` EXCEPT sklearn removes single letter words, the ALIGN method does not
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])
    sum1 = sum([vec1[x]**2 for x in list(vec1.keys())])
    sum2 = sum([vec2[x]**2 for x in list(vec2.keys())])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)
    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator


def build_composite_semantic_vector(lemma_seq,vocablist,highDimModel):
    """
    Function for producing vocablist and model is called in the main loop
    """

    # vocablist are all unique lemmas except those removed if only occurred once or user-specified setting of frequency threshold
    filter_lemma_seq = [word for word in lemma_seq if word in vocablist]
    # build composite vector via simple sum of vectors
    getComposite = [0] * len(highDimModel[vocablist[1]])
    for w1 in filter_lemma_seq:
        if w1 in highDimModel:
            semvector = highDimModel[w1]
            getComposite = getComposite + semvector
    return getComposite


def BuildSemanticModel(semantic_model_input_file,
                        pretrained_input_file,
                        output_file_directory,
                        use_pretrained_vectors=True,
                        high_sd_cutoff=3,
                        low_n_cutoff=1,
                        save_vocab_freqs=False):

    """
    Given an input file produced by the ALIGN Phase 1 functions,
    build a semantic model from all transcripts in all conversations
    in target corpus after removing high- and low-frequency words.
    High-frequency words are determined by a user-defined number of
    SDs over the mean (by default, `high_sd_cutoff=3`). Low-frequency
    words must appear over a specified number of raw occurrences
    (by default, `low_n_cutoff=1`).

    Frequency cutoffs can be removed by `high_sd_cutoff=None` and/or
    `low_n_cutoff=0`.

    Also optionally saves to `output_file_directory` a .txt list of all
    unique words and their frequency count if `save_vocab_freqs=True`
    """

    # build vocabulary list from transcripts
    data1 = pd.read_csv(semantic_model_input_file, sep='\t', encoding='utf-8')

    # get frequency count of all included words
    all_sentences = [re.sub('[^\w\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]
    all_words = list([a for b in all_sentences for a in b])
    frequency = defaultdict(int)
    for word in all_words:
        frequency[word] += 1

    # only include words whose length is greater than 1
    frequency = {word: freq for word, freq in frequency.items() if len(word) > 1}

    # NOTE: experimental feature for 0.0.12, print out list of all unique words with frequency count
    if save_vocab_freqs:
        vocabfreq_df = pd.DataFrame(list(frequency.items()), columns=["word", "count"]).sort_values(by=['count'], ascending=False)
        vocabfreq_file = os.path.join(output_file_directory,'../vocabfreqs.txt')
        vocabfreq_df.to_csv(vocabfreq_file,
                    encoding='utf-8',index=False, sep='\t')

    # only include words that occur more frequently than our cutoff (defined in occurrences)
    frequency = {word: freq for word, freq in frequency.items() if freq > low_n_cutoff}

    # if desired, remove high-frequency words (over user-defined SDs above mean)
    if high_sd_cutoff is None:
        contentWords = [word for word in list(frequency.keys())]
    else:
        getOut = np.mean(list(frequency.values()))+(np.std(list(frequency.values()))*(high_sd_cutoff))
        contentWords = list({word: freq for word, freq in frequency.items() if freq < getOut}.keys())

    # decide whether to build semantic model from scratch or load in pretrained vectors
    if not use_pretrained_vectors:
        keepSentences = [[word for word in row if word in contentWords] for row in all_sentences]
        semantic_model = word2vec.Word2Vec(all_sentences, min_count=low_n_cutoff)
    else:
        if pretrained_input_file is None:
            raise ValueError('Error! Specify path to pretrained vector file using the `pretrained_input_file` argument.')
        else:
            semantic_model = gensim.models.KeyedVectors.load_word2vec_format(pretrained_input_file, binary=True)

    # return all the content words and the trained word vectors
    word_to_vector = {}
    for word, index in semantic_model.key_to_index.items():
        word_to_vector[word] = semantic_model.vectors[index, :]
    return contentWords, word_to_vector


def LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,
                             tok2,lem2,penn_tok2,penn_lem2,
                             stan_tok1=None,stan_lem1=None,
                             stan_tok2=None,stan_lem2=None,
                             maxngram=2,
                             ignore_duplicates=True,
                             add_stanford_tags=False):

    """
    Derive lexical and part-of-speech alignment scores
    between interlocutors (suffix `1` and `2` in arguments
    passed to function).

    By default, return scores based only on Penn POS taggers.
    If desired, also return scores using Stanford tagger with
    `add_stanford_tags=True` and by providing appropriate
    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and
    `stan_lem2`.

    By default, consider only bigram when calculating
    similarity. If desired, this window may be expanded
    by changing the `maxngram` argument value.

    By default, remove exact duplicates when calculating
    similarity scores (i.e., does not consider perfectly
    mimicked lexical items between speakers). If desired,
    duplicates may be included when calculating scores by
    passing `ignore_duplicates=False`.
    """

    # create empty dictionaries for syntactic similarity
    syntax_penn_tok = {}
    syntax_penn_lem = {}

    # if desired, generate Stanford-based scores
    if add_stanford_tags:
        syntax_stan_tok = {}
        syntax_stan_lem = {}

    # create empty dictionaries for lexical similarity
    lexical_tok = {}
    lexical_lem = {}

    # cycle through all desired ngram lengths
    for ngram in range(1,maxngram+1):

        # calculate similarity for lexical ngrams (tokens and lemmas)
        [vectorT1, vectorT2] = ngram_lexical(tok1,tok2,ngramsize=ngram)
        [vectorL1, vectorL2] = ngram_lexical(lem1,lem2,ngramsize=ngram)
        lexical_tok['lexical_tok{0}'.format(ngram)] = get_cosine(vectorT1,vectorT2)
        lexical_lem['lexical_lem{0}'.format(ngram)] = get_cosine(vectorL1, vectorL2)

        # calculate similarity for Penn POS ngrams (tokens)
        [vector_penn_tok1, vector_penn_tok2] = ngram_pos(penn_tok1,penn_tok2,
                                                ngramsize=ngram,
                                                ignore_duplicates=ignore_duplicates)
        syntax_penn_tok['syntax_penn_tok{0}'.format(ngram)] = get_cosine(vector_penn_tok1,
                                                                                            vector_penn_tok2)
        # calculate similarity for Penn POS ngrams (lemmas)
        [vector_penn_lem1, vector_penn_lem2] = ngram_pos(penn_lem1,penn_lem2,
                                                              ngramsize=ngram,
                                                              ignore_duplicates=ignore_duplicates)
        syntax_penn_lem['syntax_penn_lem{0}'.format(ngram)] = get_cosine(vector_penn_lem1,
                                                                                            vector_penn_lem2)

        # if desired, also calculate using Stanford POS
        if add_stanford_tags:

            # calculate similarity for Stanford POS ngrams (tokens)
            [vector_stan_tok1, vector_stan_tok2] = ngram_pos(stan_tok1,stan_tok2,
                                                                  ngramsize=ngram,
                                                                  ignore_duplicates=ignore_duplicates)
            syntax_stan_tok['syntax_stan_tok{0}'.format(ngram)] = get_cosine(vector_stan_tok1,
                                                                                                vector_stan_tok2)

            # calculate similarity for Stanford POS ngrams (lemmas)
            [vector_stan_lem1, vector_stan_lem2] = ngram_pos(stan_lem1,stan_lem2,
                                                                  ngramsize=ngram,
                                                                  ignore_duplicates=ignore_duplicates)
            syntax_stan_lem['syntax_stan_lem{0}'.format(ngram)] = get_cosine(vector_stan_lem1,
                                                                                                vector_stan_lem2)

    # return requested information
    if add_stanford_tags:
        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,
                             syntax_stan_tok, syntax_stan_lem,
                             lexical_tok, lexical_lem]
    else:
        dictionaries_list = [syntax_penn_tok, syntax_penn_lem,
                             lexical_tok, lexical_lem]

    return dictionaries_list


def conceptualAlignment(lem1, lem2, vocablist, highDimModel):

    """
    Calculate conceptual alignment scores from list of lemmas
    from between two interocutors (suffix `1` and `2` in arguments
    passed to function) using `word2vec`.
    """

    # aggregate composite high-dimensional vectors of all words in utterance
    W2Vec1 = build_composite_semantic_vector(lem1,vocablist,highDimModel)
    W2Vec2 = build_composite_semantic_vector(lem2,vocablist,highDimModel)

    # return cosine distance alignment score; checks whether one of the vectors is composed of all zeros (occasionaly occurs when words in utterance were removed from master vocablist because of low or high frequency issues)
    if all(v == 0 for v in W2Vec1) | all(v == 0 for v in W2Vec2):
        return 0.0
    else:
        return 1 - spatial.distance.cosine(W2Vec1, W2Vec2)

def returnMultilevelAlignment(cond_info,
                                   partnerA,tok1,lem1,penn_tok1,penn_lem1,
                                   partnerB,tok2,lem2,penn_tok2,penn_lem2,
                                   vocablist, highDimModel,
                                   stan_tok1=None,stan_lem1=None,
                                   stan_tok2=None,stan_lem2=None,
                                   add_stanford_tags=False,
                                   maxngram=2,
                                   ignore_duplicates=True):

    """
    Calculate lexical, syntactic, and conceptual alignment
    between a pair of turns by individual interlocutors
    (suffix `1` and `2` in arguments passed to function),
    including leading/following comparison directionality.

    By default, return scores based only on Penn POS taggers.
    If desired, also return scores using Stanford tagger with
    `add_stanford_tags=True` and by providing appropriate
    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and
    `stan_lem2`.

    By default, consider only bigrams when calculating
    similarity. If desired, this window may be expanded
    by changing the `maxngram` argument value.

    By default, remove exact duplicates when calculating
    similarity scores (i.e., does not consider perfectly
    mimicked lexical items between speakers). If desired,
    duplicates may be included when calculating scores by
    passing `ignore_duplicates=False`.
    """

    # create empty dictionaries
    partner_direction = {}
    condition_info = {}
    cosine_semanticL = {}
    utterance_length1 = {}
    utterance_length2 = {}

    # calculate lexical and syntactic alignment
    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,
                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,
                                                 tok2=tok2,lem2=lem2,
                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,
                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,
                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,
                                                 maxngram=maxngram,
                                                 ignore_duplicates=ignore_duplicates,
                                                 add_stanford_tags=add_stanford_tags)

    # calculate conceptual alignment
    cosine_semanticL['cosine_semanticL'] = conceptualAlignment(lem1,lem2,vocablist,highDimModel)
    dictionaries_list.append(cosine_semanticL.copy())

    # determine directionality of leading/following comparison;  Note: Partner B is the lagged partner, thus, B is following A
    partner_direction['partner_direction'] = str(partnerA) + ">" + str(partnerB)
    dictionaries_list.append(partner_direction.copy())

    # add number of tokens in each utterance
    utterance_length1['utterance_length1'] = len(tok1)
    dictionaries_list.append(utterance_length1.copy())

    utterance_length2['utterance_length2'] = len(tok2)
    dictionaries_list.append(utterance_length2.copy())

    # add condition information
    condition_info['condition_info'] = cond_info
    dictionaries_list.append(condition_info.copy())

    # return alignment scores
    return dictionaries_list


def TurnByTurnAnalysis(dataframe,
                            vocablist,
                            highDimModel,
                            delay=1,
                            maxngram=2,
                            add_stanford_tags=False,
                            ignore_duplicates=True):

    """
    Calculate lexical, syntactic, and conceptual alignment
    between interlocutors over an entire conversation.
    Automatically detect individual speakers by unique
    speaker codes.

    By default, compare only adjacent turns. If desired,
    the comparison distance may be changed by increasing
    the `delay` argument.

    By default, include maximum n-gram comparison of 2. If
    desired, this may be changed by passing the appropriate
    value to the the `maxngram` argument.

    By default, return scores based only on Penn POS taggers.
    If desired, also return scores using Stanford tagger with
    `add_stanford_tags=True`.

    By default, remove exact duplicates when calculating POS
    similarity scores (i.e., does not consider perfectly
    mimicked lexical items between speakers). If desired,
    duplicates may be included when calculating scores by
    passing `ignore_duplicates=False`.
    """

    # if we don't want the Stanford tagger data, set defaults
    if not add_stanford_tags:
        stan_tok1=None
        stan_lem1=None
        stan_tok2=None
        stan_lem2=None

    # prepare the data to the appropriate type
    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086
    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086

    # if desired, prepare the Stanford tagger data
    if add_stanford_tags:
        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086
        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: list(zip(x[0::2],x[1::2]))) # thanks to https://stackoverflow.com/a/4647086

    # create lagged version of the dataframe
    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)
    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)

    # cycle through each pair of turns
    # aggregated_df = pd.DataFrame()
    tmpfiles = list()

    for i in range(0,df_original.shape[0]):

        # identify the condition for this dataframe
        cond_info = dataframe['file'].unique()
        if len(cond_info)==1:
            cond_info = str(cond_info[0])

        # break and flag error if we have more than 1 condition per dataframe
        else:
            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)

        # grab all of first participant's data
        first_row = df_original.iloc[i]
        first_partner = first_row['participant']
        tok1=first_row['token']
        lem1=first_row['lemma']
        penn_tok1=first_row['tagged_token']
        penn_lem1=first_row['tagged_lemma']

        # grab all of lagged participant's data
        lagged_row = df_lagged.iloc[i]
        lagged_partner = lagged_row['participant']
        tok2=lagged_row['token']
        lem2=lagged_row['lemma']
        penn_tok2=lagged_row['tagged_token']
        penn_lem2=lagged_row['tagged_lemma']

        # if desired, grab the Stanford tagger data for both participants
        if add_stanford_tags:
            stan_tok1=first_row['tagged_stan_token']
            stan_lem1=first_row['tagged_stan_lemma']
            stan_tok2=lagged_row['tagged_stan_token']
            stan_lem2=lagged_row['tagged_stan_lemma']

        # process multilevel alignment
        dictionaries_list=returnMultilevelAlignment(cond_info=cond_info,
                                                         partnerA=first_partner,
                                                         tok1=tok1,lem1=lem1,
                                                         penn_tok1=penn_tok1,penn_lem1=penn_lem1,
                                                         partnerB=lagged_partner,
                                                         tok2=tok2,lem2=lem2,
                                                         penn_tok2=penn_tok2,penn_lem2=penn_lem2,
                                                         vocablist=vocablist,
                                                         highDimModel=highDimModel,
                                                         stan_tok1=stan_tok1,stan_lem1=stan_lem1,
                                                         stan_tok2=stan_tok2,stan_lem2=stan_lem2,
                                                         maxngram = maxngram,
                                                         ignore_duplicates = ignore_duplicates,
                                                         add_stanford_tags = add_stanford_tags)

        # sort columns so they are in order, append data to existing structures
        next_df_line = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),
                               orient='index').transpose()

        # aggregated_df = aggregated_df.append(next_df_line) ## problematic. appending a dataframe to a dataframe.
        tmpfiles.append(next_df_line)

    # reformat turn information and add index
    aggregated_df = pd.concat(tmpfiles)
    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={"index":"time"})

    # give us our finished dataframe
    return aggregated_df


def ConvoByConvoAnalysis(dataframe,
                          maxngram=2,
                          ignore_duplicates=True,
                          add_stanford_tags=False):

    """
    Calculate analysis of multilevel similarity over
    a conversation between two interlocutors from a
    transcript dataframe prepared by Phase 1
    of ALIGN. Automatically detect speakers by unique
    speaker codes.

    By default, include maximum n-gram comparison of 2. If
    desired, this may be changed by passing the appropriate
    value to the the `maxngram` argument.

    By default, return scores based only on Penn POS taggers.
    If desired, also return scores using Stanford tagger with
    `add_stanford_tags=True`.

    By default, remove exact duplicates when calculating POS
    similarity scores (i.e., does not consider perfectly
    mimicked lexical items between speakers). If desired,
    duplicates may be included when calculating scores by
    passing `ignore_duplicates=False`.
    """

    # identify the condition for this dataframe
    cond_info = dataframe['file'].unique()
    if len(cond_info)==1:
        cond_info = str(cond_info[0])

    # break and flag error if we have more than 1 condition per dataframe
    else:
        raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)

    # if we don't want the Stanford info, set defaults
    if not add_stanford_tags:
        stan_tok1 = None
        stan_lem1 = None
        stan_tok2 = None
        stan_lem2 = None

    # identify individual interlocutors
    df_A = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[0]]
    df_B = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[1]]

    # concatenate the token, lemma, and POS information for participant A
    tok1 = [word for turn in df_A['token'] for word in turn]
    lem1 = [word for turn in df_A['lemma'] for word in turn]
    penn_tok1 = [POS for turn in df_A['tagged_token'] for POS in turn]
    penn_lem1 = [POS for turn in df_A['tagged_lemma'] for POS in turn]
    if add_stanford_tags:

        if isinstance(df_A['tagged_stan_token'][0], list):
            stan_tok1 = [POS for turn in df_A['tagged_stan_token'] for POS in turn]
            stan_lem1 = [POS for turn in df_A['tagged_stan_lemma'] for POS in turn]

        elif isinstance(df_A['tagged_stan_token'][0], str):
            stan_tok1 = pd.Series(df_A['tagged_stan_token'].values).apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
            stan_tok1 = stan_tok1.apply(lambda x: list(zip(x[0::2],x[1::2])))
            stan_tok1 = [POS for turn in stan_tok1 for POS in turn]
            stan_lem1 = pd.Series(df_A['tagged_stan_lemma'].values).apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
            stan_lem1 = stan_lem1.apply(lambda x: list(zip(x[0::2],x[1::2])))
            stan_lem1 = [POS for turn in stan_lem1 for POS in turn]

    # concatenate the token, lemma, and POS information for participant B
    tok2 = [word for turn in df_B['token'] for word in turn]
    lem2 = [word for turn in df_B['lemma'] for word in turn]
    penn_tok2 = [POS for turn in df_B['tagged_token'] for POS in turn]
    penn_lem2 = [POS for turn in df_B['tagged_lemma'] for POS in turn]
    if add_stanford_tags:

        if isinstance(df_A['tagged_stan_token'][0],list):
            stan_tok2 = [POS for turn in df_B['tagged_stan_token'] for POS in turn]
            stan_lem2 = [POS for turn in df_B['tagged_stan_lemma'] for POS in turn]

        elif isinstance(df_A['tagged_stan_token'][0], str):
            stan_tok2 = pd.Series(df_B['tagged_stan_token'].values).apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
            stan_tok2 = stan_tok2.apply(lambda x: list(zip(x[0::2],x[1::2])))
            stan_tok2 = [POS for turn in stan_tok2 for POS in turn]
            stan_lem2 = pd.Series(df_B['tagged_stan_lemma'].values).apply(lambda x: re.sub('[^\w\s]+','',x).split(' '))
            stan_lem2 = stan_lem2.apply(lambda x: list(zip(x[0::2],x[1::2])))
            stan_lem2 = [POS for turn in stan_lem2 for POS in turn]

    # process multilevel alignment
    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,
                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,
                                                 tok2=tok2,lem2=lem2,
                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,
                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,
                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,
                                                 maxngram=maxngram,
                                                 ignore_duplicates=ignore_duplicates,
                                                 add_stanford_tags=add_stanford_tags)

    # append data to existing structures
    dictionary_df = pd.DataFrame.from_dict(OrderedDict(k for num, i in enumerate(d for d in dictionaries_list) for k in sorted(i.items())),
                       orient='index').transpose()
    dictionary_df['condition_info'] = cond_info

    # return the dataframe
    return dictionary_df


def GenerateSurrogate(original_conversation_list,
                           surrogate_file_directory,
                           all_surrogates=True,
                           keep_original_turn_order=True,
                           id_separator = '\-',
                           dyad_label='dyad',
                           condition_label='cond'):

    """
    Create transcripts for surrogate pairs of
    participants (i.e., participants who did not
    genuinely interact in the experiment), which
    will later be used to generate baseline levels
    of alignment. Store surrogate files in a new
    folder each time the surrogate generation is run.

    Returns a list of all surrogate files created.

    By default, the separator between dyad ID and
    condition ID is a hyphen ('\-'). If desired,
    this may be changed in the `id_separator`
    argument.

    By default, condition IDs will be identified as
    any characters following `cond`. If desired,
    this may be changed with the `condition_label`
    argument.

    By default, dyad IDs will be identified as
    any characters following `dyad`. If desired,
    this may be changed with the `dyad_label`
    argument.

    By default, generate surrogates from all possible
    pairings. If desired, instead generate surrogates
    only from a subset of all possible pairings
    with `all_surrogates=False`.

    By default, create surrogates by retaining the
    original ordering of each surrogate partner's
    data. If desired, create surrogates by shuffling
    all turns within each surrogate partner's data
    with `keep_original_turn_order = False`.
    """

    # create a subfolder for the new set of surrogates
    import time
    new_surrogate_path = surrogate_file_directory + 'surrogate_run-' + str(time.time()) +'/'
    if not os.path.exists(new_surrogate_path):
        os.makedirs(new_surrogate_path)

    # grab condition types from each file name
    file_info = [re.sub('\.txt','',os.path.basename(file_name)) for file_name in original_conversation_list]

    # common user mistake is not including in the filename the correct dyad, condition, or separator specifications
    if (dyad_label not in file_info[0] or condition_label not in file_info[0] or id_separator.strip("\\") not in file_info[0]):
        raise Exception("You most likely have a problem with your filenames. The filename needs to include the `id_separator`, `dyad_label`, and `condition_label` specified in the parameters for the main function.")

    condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))
    files_conditions = {}
    for unique_condition in condition_ids:
        next_condition_files = [add_file for add_file in original_conversation_list if unique_condition in add_file]
        files_conditions[unique_condition] = next_condition_files

    # cycle through conditions
    for condition in list(files_conditions.keys()):

        # default: grab all possible pairs of conversations of this condition
        paired_surrogates = [pair for pair in combinations(files_conditions[condition],2)]

        # otherwise, if desired, randomly pull from all pairs to get target surrogate sample
        if not all_surrogates:
            import math
            paired_surrogates = random.sample(paired_surrogates,
                                              int(math.ceil(len(files_conditions[condition])/2)))

        # cycle through surrogate pairings
        for next_surrogate in paired_surrogates:

            # read in the files
            original_file1 = os.path.basename(next_surrogate[0])
            original_file2 = os.path.basename(next_surrogate[1])
            original_df1=pd.read_csv(next_surrogate[0], sep='\t',encoding='utf-8')
            original_df2=pd.read_csv(next_surrogate[1], sep='\t',encoding='utf-8')
            if (len(original_df1) < 1 or len(original_df2) < 1):
                raise Exception("You are attempting to process a file with no conversational turns. Double-check your files and remove any empty ones.")

            # get participants A and B from df1
            participantA_1_code = min(original_df1['participant'].unique())
            participantB_1_code = max(original_df1['participant'].unique())
            participantA_1 = original_df1[original_df1['participant'] == participantA_1_code].reset_index().rename(columns={'file': 'original_file'})
            participantB_1 = original_df1[original_df1['participant'] == participantB_1_code].reset_index().rename(columns={'file': 'original_file'})

            # get participants A and B from df2
            participantA_2_code = min(original_df2['participant'].unique())
            participantB_2_code = max(original_df2['participant'].unique())
            participantA_2 = original_df2[original_df2['participant'] == participantA_2_code].reset_index().rename(columns={'file': 'original_file'})
            participantB_2 = original_df2[original_df2['participant'] == participantB_2_code].reset_index().rename(columns={'file': 'original_file'})

            # identify truncation point for both surrogates (to have even number of turns)
            surrogateX_turns=min([participantA_1.shape[0],
                                  participantB_2.shape[0]])
            surrogateY_turns=min([participantA_2.shape[0],
                                  participantB_1.shape[0]])

            # preserve original turn order for surrogate pairs
            if keep_original_turn_order:
                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,
                                                        copy=False)
                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,
                                                        copy=False)
                surrogateX = pd.concat(
                    [surrogateX_A1, surrogateX_B2]).sort_index(
                            kind="mergesort").reset_index(
                                    drop=True).rename(
                                        columns={'index': 'original_index'})

                surrogateY_A2 = participantA_2.truncate(after=surrogateY_turns-1,
                                                        copy=False)
                surrogateY_B1 = participantB_1.truncate(after=surrogateY_turns-1,
                                                        copy=False)
                surrogateY = pd.concat(
                    [surrogateY_A2, surrogateY_B1]).sort_index(
                            kind="mergesort").reset_index(
                                    drop=True).rename(
                                            columns={'index': 'original_index'})

            # otherwise, if desired, just shuffle all turns within participants
            else:

                # shuffle for first surrogate pairing
                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)
                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)
                surrogateX = pd.concat([surrogateX_A1,surrogateX_B2]).sort_index(kind="mergesort").reset_index(drop=True).rename(columns={'index': 'original_index'})

                # and for second surrogate pairing
                surrogateY_A2 = participantA_2.truncate(after=surrogateY_turns-1,copy=False).sample(frac=1).reset_index(drop=True)
                surrogateY_B1 = participantB_1.truncate(after=surrogateY_turns-1,copy=False).sample(frac=1).reset_index(drop=True)
                surrogateY = pd.concat([surrogateY_A2,surrogateY_B1]).sort_index(kind="mergesort").reset_index(drop=True).rename(columns={'index': 'original_index'})

            # create filename for our surrogate file
            original_dyad1 = re.findall(dyad_label+'[^'+id_separator+']*',original_file1)[0]
            original_dyad2 = re.findall(dyad_label+'[^'+id_separator+']*',original_file2)[0]
            surrogateX['file'] = original_dyad1 + '-' + original_dyad2 + '-' + condition
            surrogateY['file'] = original_dyad2 + '-' + original_dyad1 + '-' + condition
            nameX='SurrogatePair-'+original_dyad1+'A'+'-'+original_dyad2+'B'+'-'+condition+'.txt'
            nameY='SurrogatePair-'+original_dyad2+'A'+'-'+original_dyad1+'B'+'-'+condition+'.txt'

            # save to file
            surrogateX.to_csv(new_surrogate_path + nameX, encoding='utf-8',index=False,sep='\t')
            surrogateY.to_csv(new_surrogate_path + nameY, encoding='utf-8',index=False,sep='\t')

    # return list of all surrogate files
    return glob.glob(new_surrogate_path+"*.txt")


def calculate_alignment(input_files,
                        output_file_directory,
                        semantic_model_input_file,
                        pretrained_input_file,
                        high_sd_cutoff=3,
                        low_n_cutoff=1,
                        delay=1,
                        maxngram=2,
                        use_pretrained_vectors=True,
                        ignore_duplicates=True,
                        add_stanford_tags=False,
                        input_as_directory=True,
                        save_vocab_freqs=False):


    # grab the files in the list
    if not input_as_directory:
        root = './new_data_files_output/'
        file_list = [root+i for i in os.listdir('./new_data_files_output') if '.txt' in i]
    else:
        root = './new_data_files_output/'
        file_list = [root+i for i in os.listdir('./new_data_files_output') if '.txt' in i]

    # build the semantic model to be used for all conversations
    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,
                                                       pretrained_input_file=pretrained_input_file,
                                                       use_pretrained_vectors=use_pretrained_vectors,
                                                       high_sd_cutoff=high_sd_cutoff,
                                                       low_n_cutoff=low_n_cutoff,
                                                       save_vocab_freqs=save_vocab_freqs,
                                                       output_file_directory=output_file_directory)

    # create containers for alignment values
    tempT2T = list()
    tempC2C = list()

    # cycle through each prepared file
    for fileName in file_list:

        # process the file if it's got a valid conversation
        dataframe=pd.read_csv(fileName, sep='\t',encoding='utf-8')
        if len(dataframe) > 1:

            # let us know which filename we're processing
            print(("Processing: "+fileName))

            # calculate turn-by-turn alignment scores
            xT2T=TurnByTurnAnalysis(dataframe=dataframe,
                                         delay=delay,
                                         maxngram=maxngram,
                                         vocablist=vocablist,
                                         highDimModel=highDimModel,
                                         add_stanford_tags=add_stanford_tags,
                                         ignore_duplicates=ignore_duplicates)
            tempT2T.append(xT2T)

            # calculate conversation-level alignment scores
            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,
                                             maxngram = maxngram,
                                             ignore_duplicates=ignore_duplicates,
                                             add_stanford_tags = add_stanford_tags)
            tempC2C.append(xT2T)

        # if it's invalid, let us know
        else:
            print(("Invalid file: "+fileName))

    # update final dataframes
    AlignmentT2T = pd.concat(tempT2T)
    real_final_turn_df = AlignmentT2T.reset_index(drop=True)
    AlignmentC2C = pd.concat(tempC2C)
    real_final_convo_df = AlignmentC2C.reset_index(drop=True)

    # export the final files
    real_final_turn_df.to_csv(output_file_directory+"AlignmentT2T.txt",
                      encoding='utf-8', index=False, sep='\t')
    real_final_convo_df.to_csv(output_file_directory+"AlignmentC2C.txt",
                       encoding='utf-8', index=False, sep='\t')

    # display the info, too
    return real_final_turn_df, real_final_convo_df


def calculate_baseline_alignment(input_files,
                                 surrogate_file_directory,
                                 output_file_directory,
                                 semantic_model_input_file,
                                 pretrained_input_file,
                                 high_sd_cutoff=3,
                                 low_n_cutoff=1,
                                 id_separator='\-',
                                 condition_label='cond',
                                 dyad_label='dyad',
                                 all_surrogates=True,
                                 keep_original_turn_order=True,
                                 delay=1,
                                 maxngram=2,
                                 use_pretrained_vectors=True,
                                 ignore_duplicates=True,
                                 add_stanford_tags=False,
                                 input_as_directory=True,
                                 save_vocab_freqs=False):

    # grab the files in the input list
    if not input_as_directory:
        root = './new_data_files_output/'
        file_list = [root+i for i in os.listdir('./new_data_files_output') if '.txt' in i]
    else:
        root = './new_data_files_output/'
        file_list = [root+i for i in os.listdir('./new_data_files_output') if '.txt' in i]

    # create a surrogate file list
    surrogate_file_list = GenerateSurrogate(
                            original_conversation_list=file_list,
                            surrogate_file_directory=surrogate_file_directory,
                            all_surrogates=all_surrogates,
                            id_separator=id_separator,
                            condition_label=condition_label,
                            dyad_label=dyad_label,
                            keep_original_turn_order=keep_original_turn_order)

    # build the semantic model to be used for all conversations
    [vocablist, highDimModel] = BuildSemanticModel(
                            semantic_model_input_file=semantic_model_input_file,
                            pretrained_input_file=pretrained_input_file,
                            use_pretrained_vectors=use_pretrained_vectors,
                            high_sd_cutoff=high_sd_cutoff,
                            low_n_cutoff=low_n_cutoff,
                            save_vocab_freqs=save_vocab_freqs,
                            output_file_directory=output_file_directory)

    # create containers for alignment values
    tempT2T = list()
    tempC2C = list()

    # cycle through the files
    for fileName in surrogate_file_list:

        # process the file if it's got a valid conversation
        dataframe=pd.read_csv(fileName, sep='\t',encoding='utf-8')
        if len(dataframe) > 1:

            # let us know which filename we're processing
            print(("Processing: "+fileName))

            # calculate turn-by-turn alignment scores
            xT2T=TurnByTurnAnalysis(dataframe=dataframe,
                                         delay=delay,
                                         maxngram=maxngram,
                                         vocablist=vocablist,
                                         highDimModel=highDimModel,
                                         add_stanford_tags = add_stanford_tags,
                                         ignore_duplicates = ignore_duplicates)
            tempT2T.append(xT2T)

            # calculate conversation-level alignment scores
            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,
                                             maxngram = maxngram,
                                             ignore_duplicates=ignore_duplicates,
                                             add_stanford_tags = add_stanford_tags)
            tempC2C.append(xC2C)

        # if it's invalid, let us know
        else:
            print(("Invalid file: "+fileName))

    # update final dataframes
    AlignmentT2T = pd.concat(tempT2T)
    surrogate_final_turn_df = AlignmentT2T.reset_index(drop=True)
    AlignmentC2C = pd.concat(tempC2C)
    surrogate_final_convo_df = AlignmentC2C.reset_index(drop=True)

    # export the final files
    surrogate_final_turn_df.to_csv(output_file_directory+"AlignmentT2T_Surrogate.txt",
                      encoding='utf-8',index=False,sep='\t')
    surrogate_final_convo_df.to_csv(output_file_directory+"AlignmentC2C_Surrogate.txt",
                       encoding='utf-8',index=False,sep='\t')

    # display the info, too
    return surrogate_final_turn_df, surrogate_final_convo_df

